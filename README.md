Title of the Project: OMNIVISION: Image Captioning Model

Abstract:

Visually impaired individuals often struggle with independence in their daily lives, whichcan lead to diminished confidence and challenges in securing employment. This project,OmniVision, seeks to transform this dependence into independence by developing an advanced image captioning model.  By integratinga text-to-speech engine, OmniVision offers immediate auditory feedback, enhancing the user's ability to navigate and understand their environment. The ultimate goal is to empower visually impaired persons with greater self reliance and open up new opportunities for employment, thereby improving their overall quality of life.

Introduction to the Project:

The OMNIVISION is an innovation image captioning model developed using python libraries TensorFlow. The main goal of this model is to generate a one- line description of a given image. This project aims to assist visually impaired individuals by providing audio descriptions of images, thereby enhancing their understanding of their surroundings. Designed to assist visually impaired individuals, it converts these captions into audio output, thereby enabling better understanding of their surroundings. With a focus on high accuracy and real-time processing, OmniVision leverages advanced algorithms in computer vision and natural language processing to create a seamless user experience. This project also explores applications in various fields, including law enforcement, healthcare, and crowd counting.

Statement about the problem:

Visually Impaired persons face a lack of independence in their daily life resulting in a lack of confidence and unemployability. The current project aims to change this dependence into
independence and provide them with the possibility of employment.

System Requirement specification:

Software Requirement :Visual studio code (React), Python Libraries, Text-to-Speech (TTS) Engine, TensorFlow.

Methodology:

To perform this step, define the scope of the applied research clearly, focusing on developing and testing the image captioning model with native language audio output. Recruit a diverse
sample of visually impaired participants through local organizations, community groups, or online platforms. 

Link : https://drive.google.com/file/d/136UHHs1k6AmnQ9RJiTsio9fPeKlNI0NS/view?usp=sharing

